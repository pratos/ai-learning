services:
  ai-learning:
    build: .
    container_name: ai-learning-train
    ipc: host
    shm_size: '2gb'
    ulimits:
      memlock: -1
      stack: 67108864
    environment:
      - NVIDIA_VISIBLE_DEVICES=all  # Use '0' for single GPU
      - PYTHONPATH=/app
      - UV_LINK_MODE=copy
    volumes:
      # Mount source code for development
      - .:/app
      # Exclude virtual environment to use container's one
      - /app/.venv
      # Persist data directory
      - ./data:/app/data
      # Persist logs and model checkpoints
      - ./logs:/app/logs
      # Optional: Mount cache directories
      - ~/.cache:/root/.cache
    ports:
      # TensorBoard
      - "6006:6006"
      # Jupyter (optional)
      - "8888:8888"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    tty: true
    stdin_open: true
    command: bash

  # TensorBoard service (no GPU needed)
  tensorboard:
    build: .
    container_name: ai-learning-tensorboard
    environment:
      - UV_LINK_MODE=copy
    volumes:
      - ./logs:/app/logs
    ports:
      - "6007:6006"
    command: tensorboard --logdir=/app/logs --host=0.0.0.0 --port=6006
    depends_on:
      - ai-learning

  # NVTX Profiling service (for advanced profiling)
  profiling:
    build: .
    container_name: ai-learning-profiling
    ipc: host
    shm_size: '4gb'  # Increased shared memory for profiling
    ulimits:
      memlock: -1
      stack: 67108864
    environment:
      - NVIDIA_VISIBLE_DEVICES=all  # Use '0' for single GPU
      - PYTHONPATH=/app
      - UV_LINK_MODE=copy
      # Reduce PyTorch memory fragmentation during profiling
      - PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128
      - OMP_NUM_THREADS=1  # Reduce CPU thread contention
      - CUDA_LAUNCH_BLOCKING=0  # Allow async CUDA operations
      # Improve symbol resolution for nsys profiling
      - LD_PRELOAD=
      - NSYS_NVTX_PROFILER_REGISTER_ONLY=0
      # Help with process tracking
      - PYTHONUNBUFFERED=1
    volumes:
      - .:/app
      # Exclude virtual environment to use container's one
      - /app/.venv
      - ./data:/app/data
      - ./logs:/app/logs
      # Mount for profiling output
      - ./profiling_output:/app/profiling_output
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    tty: true
    stdin_open: true
    command: bash