#!/bin/bash

# AI Learning - Docker Runtime (uv-powered)

set -e

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Enable Docker Bake for better build performance
export COMPOSE_BAKE=true

# Function to show Docker banner
show_docker_banner() {
    echo -e "${BLUE}üê≥ AI Learning - Docker Runtime (uv-powered)${NC}"
    echo "============================================="
}

# Function to show local banner
show_local_banner() {
    echo -e "${GREEN}üè† AI Learning - Local Development (uv-powered)${NC}"
    echo "==============================================="
}

# Function to detect Linux distribution
detect_distro() {
    if [ -f /etc/os-release ]; then
        . /etc/os-release
        echo $ID
    elif [ -f /etc/redhat-release ]; then
        echo "rhel"
    elif [ -f /etc/debian_version ]; then
        echo "debian"
    else
        echo "unknown"
    fi
}

# Function to install Docker
install_docker() {
    echo -e "${YELLOW}üîß Installing Docker...${NC}"
    
    local distro=$(detect_distro)
    
    case $distro in
        "ubuntu"|"debian")
            # Update package index
            sudo apt-get update
            
            # Install required packages
            sudo apt-get install -y \
                ca-certificates \
                curl \
                gnupg \
                lsb-release
            
            # Add Docker's official GPG key
            sudo mkdir -p /etc/apt/keyrings
            curl -fsSL https://download.docker.com/linux/$distro/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg
            
            # Set up the repository
            echo \
                "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/$distro \
                $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
            
            # Update package index again
            sudo apt-get update
            
            # Install Docker Engine
            sudo apt-get install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
            
            # Add user to docker group
            sudo usermod -aG docker $USER
            
            echo -e "${GREEN}‚úÖ Docker installed successfully${NC}"
            echo -e "${YELLOW}‚ö†Ô∏è  Please log out and log back in for group changes to take effect${NC}"
            ;;
        "rhel"|"centos"|"fedora")
            # Install using yum/dnf
            if command -v dnf &> /dev/null; then
                PKG_MGR="dnf"
            else
                PKG_MGR="yum"
            fi
            
            sudo $PKG_MGR install -y yum-utils
            sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
            sudo $PKG_MGR install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
            
            # Start and enable Docker
            sudo systemctl start docker
            sudo systemctl enable docker
            
            # Add user to docker group
            sudo usermod -aG docker $USER
            
            echo -e "${GREEN}‚úÖ Docker installed successfully${NC}"
            ;;
        *)
            echo -e "${RED}‚ùå Unsupported distribution: $distro${NC}"
            echo "Please install Docker manually: https://docs.docker.com/engine/install/"
            exit 1
            ;;
    esac
}

# Function to install Docker Compose standalone
install_docker_compose() {
    echo -e "${YELLOW}üîß Installing Docker Compose...${NC}"
    
    # Get latest version
    local compose_version=$(curl -s https://api.github.com/repos/docker/compose/releases/latest | grep 'tag_name' | cut -d\" -f4)
    
    if [ -z "$compose_version" ]; then
        echo -e "${YELLOW}‚ö†Ô∏è  Could not detect latest version, using v2.27.0${NC}"
        compose_version="v2.27.0"
    fi
    
    # Download and install
    sudo curl -L "https://github.com/docker/compose/releases/download/${compose_version}/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
    sudo chmod +x /usr/local/bin/docker-compose
    
    # Create symlink if needed
    if [ ! -f /usr/bin/docker-compose ]; then
        sudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose
    fi
    
    echo -e "${GREEN}‚úÖ Docker Compose ${compose_version} installed successfully${NC}"
}

# Function to install NVIDIA Docker
install_nvidia_docker() {
    echo -e "${YELLOW}üîß Installing NVIDIA Docker runtime...${NC}"
    
    local distro=$(detect_distro)
    
    case $distro in
        "ubuntu"|"debian")
            # Add NVIDIA package repositories
            distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
            
            # Handle GPG key - don't prompt if it exists
            if [ ! -f /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg ]; then
                curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg
            else
                echo -e "${BLUE}‚ÑπÔ∏è  NVIDIA GPG key already exists, skipping...${NC}"
            fi
            
            # Handle repository list - don't duplicate if it exists
            if [ ! -f /etc/apt/sources.list.d/nvidia-container-toolkit.list ]; then
                curl -s -L https://nvidia.github.io/libnvidia-container/$distribution/libnvidia-container.list | \
                    sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
                    sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list > /dev/null
            else
                echo -e "${BLUE}‚ÑπÔ∏è  NVIDIA repository already configured, skipping...${NC}"
            fi
            
            # Update package index
            sudo apt-get update
            
            # Install NVIDIA Docker
            sudo apt-get install -y nvidia-docker2
            
            echo -e "${GREEN}‚úÖ NVIDIA Docker runtime installed successfully${NC}"
            echo -e "${YELLOW}‚ö†Ô∏è  Please restart Docker manually if needed: sudo systemctl restart docker${NC}"
            ;;
        "rhel"|"centos"|"fedora")
            # Add NVIDIA package repositories
            distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
            
            # Handle repository file - don't duplicate if it exists
            if [ ! -f /etc/yum.repos.d/nvidia-container-toolkit.repo ]; then
                curl -s -L https://nvidia.github.io/libnvidia-container/$distribution/nvidia-container-toolkit.repo | \
                    sudo tee /etc/yum.repos.d/nvidia-container-toolkit.repo > /dev/null
            else
                echo -e "${BLUE}‚ÑπÔ∏è  NVIDIA repository already configured, skipping...${NC}"
            fi
            
            # Install NVIDIA Docker
            if command -v dnf &> /dev/null; then
                sudo dnf install -y nvidia-docker2
            else
                sudo yum install -y nvidia-docker2
            fi
            
            echo -e "${GREEN}‚úÖ NVIDIA Docker runtime installed successfully${NC}"
            echo -e "${YELLOW}‚ö†Ô∏è  Please restart Docker manually if needed: sudo systemctl restart docker${NC}"
            ;;
        *)
            echo -e "${RED}‚ùå Unsupported distribution for NVIDIA Docker: $distro${NC}"
            echo "Please install NVIDIA Docker manually: https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html"
            exit 1
            ;;
    esac
}

# Function to ensure Docker is running
ensure_docker_running() {
    local command_name="${1:-}"
    
    # Check if Docker is running
    if ! docker info > /dev/null 2>&1; then
        echo -e "${RED}‚ùå Docker is not running. Please start Docker first.${NC}"
        echo "Try: sudo systemctl start docker"
        exit 1
    fi
    
    # Check NVIDIA runtime only for commands that need it (skip for cleanup)
    if [[ "$command_name" != "cleanup" ]]; then
        # Check if nvidia-docker is available
        if ! docker run --rm --gpus all nvidia/cuda:11.0-base-ubuntu20.04 nvidia-smi > /dev/null 2>&1; then
            echo -e "${YELLOW}‚ö†Ô∏è  NVIDIA Docker runtime not available.${NC}"
            echo -e "${BLUE}‚ÑπÔ∏è  GPU drivers will be provided by the NVIDIA PyTorch container.${NC}"
            
            # Check if nvidia-docker2 package is installed but not working
            nvidia_docker_installed=false
            if command -v apt-get &> /dev/null && dpkg -l | grep -q nvidia-docker2; then
                nvidia_docker_installed=true
                echo -e "${BLUE}‚ÑπÔ∏è  nvidia-docker2 package found${NC}"
            elif (command -v yum &> /dev/null || command -v dnf &> /dev/null) && rpm -qa | grep -q nvidia-docker2; then
                nvidia_docker_installed=true
                echo -e "${BLUE}‚ÑπÔ∏è  nvidia-docker2 package found${NC}"
            fi
            
            if [ "$nvidia_docker_installed" = true ]; then
                echo -e "${YELLOW}üîß NVIDIA Docker package installed but not working. This could be due to:${NC}"
                echo "  ‚Ä¢ Docker daemon needs to reload configuration"
                echo "  ‚Ä¢ Missing NVIDIA drivers on host"
                echo "  ‚Ä¢ Docker runtime configuration issues"
                echo ""
                echo -e "${BLUE}üí° You can try:${NC}"
                echo "  ‚Ä¢ sudo systemctl reload docker    # Reload config without restart"
                echo "  ‚Ä¢ nvidia-smi                      # Check if NVIDIA drivers are working"
                echo ""
                echo -e "${YELLOW}‚ö†Ô∏è  Continuing without NVIDIA runtime - GPU will work via container drivers${NC}"
            else
                echo -e "${BLUE}üì¶ Installing NVIDIA Docker runtime...${NC}"
                install_nvidia_docker
                echo -e "${BLUE}üîÑ NVIDIA Docker runtime installed.${NC}"
                
                # Test again after installation
                if docker run --rm --gpus all nvidia/cuda:11.0-base-ubuntu20.04 nvidia-smi > /dev/null 2>&1; then
                    echo -e "${GREEN}‚úÖ NVIDIA Docker runtime is now working${NC}"
                else
                    echo -e "${YELLOW}‚ö†Ô∏è  NVIDIA Docker runtime still not working after installation${NC}"
                    echo -e "${BLUE}‚ÑπÔ∏è  This is often due to missing NVIDIA drivers on the host${NC}"
                    echo -e "${BLUE}‚ÑπÔ∏è  GPU functionality will be provided by the container's NVIDIA PyTorch base image${NC}"
                fi
            fi
        else
            echo -e "${GREEN}‚úÖ NVIDIA Docker runtime already working${NC}"
        fi
        
        echo -e "${GREEN}‚úÖ Docker and NVIDIA runtime are available${NC}"
    else
        echo -e "${GREEN}‚úÖ Docker is running and ready for cleanup${NC}"
    fi
}

# Function to ensure Docker is installed and available
ensure_docker_installed() {
    # Check if Docker is installed
    if ! command -v docker &> /dev/null; then
        echo -e "${YELLOW}‚ö†Ô∏è  Docker not found. Installing Docker...${NC}"
        install_docker
        echo -e "${BLUE}üîÑ Please run this script again after logging out and back in.${NC}"
        exit 0
    fi

    # Check if Docker Compose is available (try plugin first, then standalone)
    if ! docker compose version &> /dev/null && ! docker-compose version &> /dev/null; then
        echo -e "${YELLOW}‚ö†Ô∏è  Docker Compose not found. Installing Docker Compose...${NC}"
        install_docker_compose
        
        # Verify installation worked (check both plugin and standalone)
        if ! docker compose version &> /dev/null && ! docker-compose version &> /dev/null; then
            echo -e "${RED}‚ùå Docker Compose installation failed. Please install manually.${NC}"
            echo "Visit: https://docs.docker.com/compose/install/"
            exit 1
        fi
        
        echo -e "${GREEN}‚úÖ Docker Compose verified working${NC}"
    fi
}

# Function to get the correct docker compose command
get_compose_cmd() {
    if docker compose version &> /dev/null; then
        echo "docker compose"
    elif docker-compose version &> /dev/null; then
        echo "docker-compose"
    else
        echo "docker compose" # fallback
    fi
}

# Function to build Docker image when needed
build_image_if_needed() {
    echo -e "${YELLOW}üî® Building Docker image with uv...${NC}"
    $(get_compose_cmd) build ai-learning
}

# Unified function to run training with optional profiling
run_unified_training() {
    local model_name=${1:-"NVTXVanillaAutoencoder"}
    local epochs=${2:-10}
    local batch_size=${3:-64}
    local enable_profiling=${4:-false}
    local job_id=${5:-"training"}
    local num_workers=${6:-0}
    
    build_image_if_needed
    
    # Determine if this is a profiling run
    if [[ "$enable_profiling" == "true" ]]; then
        echo -e "${YELLOW}üîç Training with profiling:${NC}"
        echo "  Model: $model_name"
        echo "  Epochs: $epochs"
        echo "  Batch size: $batch_size"
        echo "  Job ID: $job_id"
        echo "  Workers: $num_workers"
        echo -e "${BLUE}üìÅ Output: ./profiling_output/${job_id}_${model_name}_${epochs}ep.nsys-rep${NC}"
        echo -e "${BLUE}üìä GPU metrics: Graphics, DRAM, NVLink, PCIe, SM, Tensor Core${NC}"
        
        # Check if model supports NVTX for proper profiling
        local use_nsys_profiling="false"
        if [[ "$model_name" == *"NVTX"* ]]; then
            use_nsys_profiling="true"
        fi
        
        if [[ "$use_nsys_profiling" == "true" ]]; then
            # Use nsys profiling for NVTX models
            $(get_compose_cmd) run --rm profiling \
                nsys profile \
                --trace=nvtx,cuda,osrt,cublas,cudnn \
                --backtrace=dwarf \
                --sample=cpu \
                --cpuctxsw=process-tree \
                --force-overwrite=true \
                --pytorch=autograd-shapes-nvtx \
                --cuda-memory-usage=true \
                --wait=all \
                --delay=2 \
                --duration=300 \
                --output=/app/profiling_output/${job_id}_${model_name}_${epochs}ep \
                python scripts/profile_training.py profile \
                --model-name "$model_name" \
                --max-epochs $epochs \
                --batch-size $batch_size \
                --num-workers $num_workers \
                --experiment-name "$job_id" \
                --enable-nvtx
        else
            # Regular profiling for non-NVTX models
            $(get_compose_cmd) run --rm ai-learning \
                python scripts/profile_training.py profile \
                --model-name "$model_name" \
                --max-epochs $epochs \
                --batch-size $batch_size \
                --num-workers $num_workers \
                --experiment-name "$job_id" \
                --no-enable-nvtx
        fi
    else
        echo -e "${BLUE}üöÄ Starting standard training:${NC}"
        echo "  Model: $model_name"
        echo "  Epochs: $epochs"
        echo "  Batch size: $batch_size"
        echo "  Workers: $num_workers"
        
        $(get_compose_cmd) run --rm ai-learning \
            python scripts/profile_training.py train \
            --model-name "$model_name" \
            --max-epochs $epochs \
            --batch-size $batch_size \
            --num-workers $num_workers \
            --experiment-name "$job_id"
    fi
}

# Function to list available models
list_available_models() {
    build_image_if_needed
    
    echo -e "${BLUE}üìã Available Models:${NC}"
    $(get_compose_cmd) run --rm ai-learning \
        python scripts/profile_training.py list-models
}

# Function to export requirements.txt with only pyproject.toml packages and their versions (runs locally)
export_requirements() {
    echo -e "${YELLOW}üì¶ Exporting requirements.txt from pyproject.toml packages (locally)...${NC}"
    
    # Check if uv is available
    if ! command -v uv &> /dev/null; then
        echo -e "${RED}‚ùå uv not found. Installing uv...${NC}"
        echo -e "${BLUE}‚ÑπÔ∏è  Installing uv via curl...${NC}"
        curl -LsSf https://astral.sh/uv/install.sh | sh
        
        # Add both possible installation paths to PATH for current session
        export PATH="$HOME/.local/bin:$HOME/.cargo/bin:$PATH"
        
        # Check again
        if ! command -v uv &> /dev/null; then
            echo -e "${RED}‚ùå uv installation failed. Please install manually:${NC}"
            echo "curl -LsSf https://astral.sh/uv/install.sh | sh"
            echo -e "${BLUE}üí° Then add to PATH: export PATH=\"\$HOME/.local/bin:\$PATH\"${NC}"
            echo -e "${BLUE}üí° Or restart your shell to pick up the PATH changes${NC}"
            return 1
        fi
        
        echo -e "${GREEN}‚úÖ uv installed successfully${NC}"
    fi
    
    # List of packages typically pre-installed in NGC PyTorch containers
    local ngc_packages=(
        "torch" "torchvision" "torchaudio" "transformers" "tensorboard"
        "matplotlib" "numpy" "scipy" "pandas" "pillow" "opencv-python" 
        "opencv-python-headless" "scikit-learn" "jupyter" "ipython" 
        "notebook" "jupyterlab" "seaborn" "plotly" "bokeh" "h5py" 
        "pytables" "numba" "cupy" "cugraph" "cudf" "dask" "rapids"
        "requests" "urllib3" "certifi" "charset-normalizer" "idna"
        "setuptools" "wheel" "pip" "click" "typing-extensions" "packaging"
        "pyyaml" "tqdm" "psutil" "six" "python-dateutil" "pytz"
        "jinja2" "markupsafe" "werkzeug" "itsdangerous" "blinker"
        "colorama" "rich" "typer"
    )
    
    # Extract package names from pyproject.toml
    echo -e "${BLUE}üìã Reading packages from pyproject.toml...${NC}"
    local pyproject_packages=()
    
    # Parse dependencies from pyproject.toml
    while IFS= read -r line; do
        # Extract package name from pyproject.toml dependency line
        # Example: "click==8.1.6", -> "click"
        # Example: "lightning>=2.5.1.post0", -> "lightning"
        if [[ $line =~ \"([^\"]+)\" ]]; then
            # Extract the full dependency string (e.g., "click==8.1.6")
            full_dep="${BASH_REMATCH[1]}"
            # Extract just the package name (before any version specifier)
            package_name=$(echo "$full_dep" | sed 's/[><=!].*//')
            if [[ -n "$package_name" ]]; then
                pyproject_packages+=("$package_name")
                echo -e "${BLUE}üì¶ Found in pyproject.toml: $package_name${NC}"
            fi
        fi
    done < <(grep -A 20 "dependencies = \[" pyproject.toml | grep '"' | head -20)
    
    if [ ${#pyproject_packages[@]} -eq 0 ]; then
        echo -e "${RED}‚ùå No packages found in pyproject.toml dependencies${NC}"
        return 1
    fi
    
    echo -e "${BLUE}‚ÑπÔ∏è  Found ${#pyproject_packages[@]} packages in pyproject.toml${NC}"
    
    # Fix permissions if needed
    if [ -d "iv_workshops.egg-info" ] && [ ! -w "iv_workshops.egg-info" ]; then
        echo -e "${YELLOW}‚ö†Ô∏è  Fixing permissions for iv_workshops.egg-info...${NC}"
        sudo chown -R $(whoami):$(whoami) iv_workshops.egg-info/ || true
    fi
    
    # Export all requirements to get version information
    echo -e "${BLUE}üìã Exporting full requirements from uv to get version info...${NC}"
    if ! uv export --format requirements-txt --no-hashes > requirements_full.txt; then
        echo -e "${RED}‚ùå Failed to export requirements. Trying to fix permissions...${NC}"
        
        # Remove problematic egg-info and try again
        rm -rf iv_workshops.egg-info/
        
        if ! uv export --format requirements-txt --no-hashes > requirements_full.txt; then
            echo -e "${RED}‚ùå Still failing. Please run manually:${NC}"
            echo "rm -rf iv_workshops.egg-info/"
            echo "uv export --format requirements-txt --no-hashes > requirements_full.txt"
            return 1
        fi
    fi
    
    # Create filtered requirements.txt with only pyproject.toml packages (excluding NGC pre-installed)
    echo -e "${BLUE}üîç Extracting versions for pyproject.toml packages (excluding NGC packages)...${NC}"
    cat > requirements.txt << EOF
# Project dependencies from pyproject.toml with their locked versions
# Generated on $(date)
# Only includes packages from pyproject.toml that are NOT pre-installed in NGC containers
# NGC containers already include: torch, torchvision, transformers, tensorboard, matplotlib, etc.
EOF
    
    echo "" >> requirements.txt
    
    # Extract versions for pyproject.toml packages only, excluding NGC packages
    for pkg in "${pyproject_packages[@]}"; do
        # Check if package is in NGC pre-installed list
        local is_ngc_package=false
        for ngc_pkg in "${ngc_packages[@]}"; do
            if [[ "${pkg,,}" == "${ngc_pkg,,}" ]]; then  # Case insensitive comparison
                is_ngc_package=true
                echo -e "${YELLOW}‚è≠Ô∏è  Skipping NGC pre-installed package: $pkg${NC}"
                break
            fi
        done
        
        # Only process if not an NGC package
        if [[ "$is_ngc_package" == "false" ]]; then
            # Find the package in the full requirements file (case insensitive)
            local found_line=$(grep -i "^${pkg}==" requirements_full.txt | head -1)
            
            if [ -n "$found_line" ]; then
                echo -e "${GREEN}‚úÖ Adding: $found_line${NC}"
                echo "$found_line" >> requirements.txt
            else
                # Try with different name variations (e.g., opencv-python-headless)
                local alt_line=$(grep -i "^.*${pkg}.*==" requirements_full.txt | head -1)
                if [ -n "$alt_line" ]; then
                    echo -e "${GREEN}‚úÖ Adding: $alt_line${NC}"
                    echo "$alt_line" >> requirements.txt
                else
                    echo -e "${YELLOW}‚ö†Ô∏è  Package $pkg not found in installed packages${NC}"
                fi
            fi
        fi
    done
    
    # Show summary
    echo ""
    echo -e "${BLUE}üìä Export Summary:${NC}"
    echo -e "${BLUE}Packages in pyproject.toml: ${#pyproject_packages[@]}${NC}"
    echo -e "${BLUE}NGC pre-installed packages skipped: $((${#pyproject_packages[@]} - $(grep -v '^#' requirements.txt | grep -v '^$' | wc -l)))${NC}"
    echo -e "${BLUE}Packages exported: $(grep -v '^#' requirements.txt | grep -v '^$' | wc -l)${NC}"
    echo ""
    
    # Cleanup
    rm -f requirements_full.txt
    
    echo -e "${GREEN}‚úÖ Requirements exported to requirements.txt${NC}"
    echo -e "${BLUE}üí° Only packages from pyproject.toml that are NOT pre-installed in NGC containers${NC}"
    
    # Show the generated file
    if [ -f requirements.txt ]; then
        echo ""
        echo -e "${YELLOW}üìã Generated requirements.txt:${NC}"
        echo "----------------------------------------"
        cat requirements.txt
        echo "----------------------------------------"
        echo -e "${BLUE}Total packages: $(grep -v '^#' requirements.txt | grep -v '^$' | wc -l)${NC}"
        
        # Show a note about potential conflicts
        if [ $(grep -v '^#' requirements.txt | grep -v '^$' | wc -l) -eq 0 ]; then
            echo ""
            echo -e "${GREEN}üéâ All your pyproject.toml packages are pre-installed in NGC containers!${NC}"
            echo -e "${BLUE}üí° Your Docker build should be very fast with no additional packages to install.${NC}"
        else
            echo ""
            echo -e "${YELLOW}‚ö†Ô∏è  If you encounter version conflicts during Docker build:${NC}"
            echo -e "${BLUE}   1. The NGC container may have different versions of these packages${NC}"
            echo -e "${BLUE}   2. Consider updating pyproject.toml to match NGC versions${NC}"
            echo -e "${BLUE}   3. Or remove version pins (use >= instead of ==) for flexibility${NC}"
        fi
    fi
}

# Function to start TensorBoard
start_tensorboard() {
    build_image_if_needed
    
    echo -e "${BLUE}üìä Starting TensorBoard...${NC}"
    $(get_compose_cmd) up -d tensorboard
    echo -e "${GREEN}‚úÖ TensorBoard running at http://localhost:6007${NC}"
}

# Function to start TensorBoard locally with uv
start_tensorboard_local() {
    echo -e "${BLUE}üìä Starting TensorBoard locally with uv...${NC}"
    
    # Check if uv is available
    if ! command -v uv &> /dev/null; then
        echo -e "${RED}‚ùå uv not found. Please run: $0 setup${NC}"
        echo -e "${BLUE}üí° This will install uv and set up the local environment${NC}"
        return 1
    fi
    
    # Check if virtual environment exists
    if [ ! -d ".venv" ]; then
        echo -e "${YELLOW}‚ö†Ô∏è  Virtual environment not found. Please run: $0 setup${NC}"
        echo -e "${BLUE}üí° This will create .venv and install dependencies${NC}"
        return 1
    fi
    
    # Check if logs directory exists
    if [ ! -d "logs/tensorboard_logs" ]; then
        echo -e "${YELLOW}‚ö†Ô∏è  No tensorboard logs found. Run some training first:${NC}"
        echo -e "${BLUE}üí° $0 train-local${NC}"
        echo -e "${BLUE}üí° $0 train${NC}"
        echo ""
        echo -e "${BLUE}üìÅ Creating logs/tensorboard_logs directory...${NC}"
        mkdir -p logs/tensorboard_logs
    fi
    
    echo -e "${BLUE}üöÄ Starting TensorBoard on port 6006...${NC}"
    echo -e "${BLUE}üìÅ Monitoring: ./logs/tensorboard_logs/${NC}"
    echo -e "${BLUE}üåê URL: http://localhost:6006${NC}"
    echo ""
    echo -e "${YELLOW}üí° Press Ctrl+C to stop TensorBoard${NC}"
    echo ""
    
    # Run TensorBoard with uv
    uv run tensorboard --logdir=logs/tensorboard_logs --host=0.0.0.0 --port=6006
}

# Function to start Nsight Streamer
start_nsight_streamer() {
    echo -e "${BLUE}üîç Starting NVIDIA Nsight Streamer...${NC}"
    echo -e "${YELLOW}üìä This will start a web-based profiling analysis tool${NC}"
    
    # Build the nsight streamer image
    echo -e "${YELLOW}üî® Building Nsight Streamer image...${NC}"
    $(get_compose_cmd) build nsight-streamer
    
    # Start the service
    $(get_compose_cmd) up -d nsight-streamer
    
    echo -e "${GREEN}‚úÖ Nsight Streamer running at:${NC}"
    echo -e "${BLUE}üåê Web Interface: http://localhost:8080${NC}"
    echo -e "${BLUE}üîê Username: nvidia, Password: nvidia${NC}"
    echo -e "${BLUE}üìÅ Profiling reports will be automatically detected from ./profiling_output/${NC}"
    echo ""
    echo -e "${YELLOW}üí° Tips:${NC}"
    echo "  ‚Ä¢ Run 'train-nvtx' command first to generate profiling data"
    echo "  ‚Ä¢ The web interface will auto-detect .nsys-rep files"
    echo "  ‚Ä¢ Use Chrome/Firefox for best compatibility"
    echo "  ‚Ä¢ Enable hardware acceleration for better performance"
}

# Function to stop Nsight Streamer
stop_nsight_streamer() {
    echo -e "${BLUE}üõë Stopping NVIDIA Nsight Streamer...${NC}"
    $(get_compose_cmd) stop nsight-streamer
    echo -e "${GREEN}‚úÖ Nsight Streamer stopped${NC}"
}

# Function to check what packages are actually installed in NGC container
check_ngc_packages() {
    echo -e "${YELLOW}üîç Checking packages installed in NGC PyTorch container...${NC}"
    
    build_image_if_needed
    
    echo -e "${BLUE}üìã Getting list of installed packages from container...${NC}"
    
    # Get list of installed packages with versions
    $(get_compose_cmd) run --rm ai-learning pip list --format=freeze > ngc_packages.txt
    
    echo -e "${GREEN}‚úÖ Package list saved to ngc_packages.txt${NC}"
    echo ""
    echo -e "${BLUE}üìä Summary:${NC}"
    echo -e "${BLUE}Total packages: $(wc -l < ngc_packages.txt)${NC}"
    echo ""
    
    # Check if any of our pyproject.toml packages are installed
    if [ -f pyproject.toml ]; then
        echo -e "${YELLOW}üîç Checking which pyproject.toml packages are pre-installed:${NC}"
        echo ""
        
        # Parse pyproject.toml packages
        local found_packages=()
        while IFS= read -r line; do
            if [[ $line =~ \"([^\"]+)\" ]]; then
                full_dep="${BASH_REMATCH[1]}"
                package_name=$(echo "$full_dep" | sed 's/[><=!].*//')
                if [[ -n "$package_name" ]]; then
                    # Check if package is in NGC container
                    local ngc_version=$(grep -i "^${package_name}==" ngc_packages.txt | head -1)
                    if [ -n "$ngc_version" ]; then
                        echo -e "${GREEN}‚úÖ $package_name: $ngc_version${NC}"
                        found_packages+=("$package_name")
                    else
                        echo -e "${RED}‚ùå $package_name: Not found in NGC container${NC}"
                    fi
                fi
            fi
        done < <(grep -A 20 "dependencies = \[" pyproject.toml | grep '"' | head -20)
        
        echo ""
        echo -e "${BLUE}üìä Results:${NC}"
        echo -e "${BLUE}Pre-installed: ${#found_packages[@]} packages${NC}"
        echo -e "${BLUE}Need to install: $(($(grep -A 20 "dependencies = \[" pyproject.toml | grep '"' | wc -l) - ${#found_packages[@]})) packages${NC}"
    fi
    
    echo ""
    echo -e "${YELLOW}üí° Tips:${NC}"
    echo "  ‚Ä¢ Review ngc_packages.txt to see all available packages"
    echo "  ‚Ä¢ Update your NGC package list in export_requirements() function"
    echo "  ‚Ä¢ Consider using >= instead of == for version flexibility"
    
    # Show first 20 packages as preview
    echo ""
    echo -e "${BLUE}üìã First 20 packages (preview):${NC}"
    echo "----------------------------------------"
    head -20 ngc_packages.txt
    echo "----------------------------------------"
    echo -e "${BLUE}üí° Full list in ngc_packages.txt${NC}"
}

# Function to run training locally with uv (no Docker)
run_local_training() {
    local model_name=${1:-"NVTXVanillaAutoencoder"}
    local epochs=${2:-10}
    local batch_size=${3:-64}
    local enable_profiling=${4:-false}
    local job_id=${5:-"training"}
    local num_workers=${6:-0}
    
    echo -e "${BLUE}üè† Running training locally with uv...${NC}"
    
    # Check if uv is available
    if ! command -v uv &> /dev/null; then
        echo -e "${RED}‚ùå uv not found. Please run: $0 setup${NC}"
        echo -e "${BLUE}üí° This will install uv and set up the local environment${NC}"
        return 1
    fi
    
    # Check if pyproject.toml exists
    if [ ! -f "pyproject.toml" ]; then
        echo -e "${RED}‚ùå pyproject.toml not found. Are you in the project root?${NC}"
        return 1
    fi
    
    # Check if virtual environment exists
    if [ ! -d ".venv" ]; then
        echo -e "${YELLOW}‚ö†Ô∏è  Virtual environment not found. Please run: $0 setup${NC}"
        echo -e "${BLUE}üí° This will create .venv and install dependencies${NC}"
        return 1
    fi
    
    # Determine if this is a profiling run
    if [[ "$enable_profiling" == "true" ]]; then
        echo -e "${YELLOW}üîç Training with profiling (local):${NC}"
        echo "  Model: $model_name"
        echo "  Epochs: $epochs"
        echo "  Batch size: $batch_size"
        echo "  Job ID: $job_id"
        echo "  Workers: $num_workers"
        echo -e "${BLUE}üìÅ Output: ./profiling_output/${job_id}_${model_name}_${epochs}ep.json${NC}"
        echo -e "${YELLOW}‚ö†Ô∏è  Note: NVTX profiling requires Docker. Use 'train' command for full profiling.${NC}"
        
        # Run profiling locally (without nsys/NVTX)
        uv run python scripts/profile_training.py profile \
            --model-name "$model_name" \
            --max-epochs $epochs \
            --batch-size $batch_size \
            --num-workers $num_workers \
            --experiment-name "$job_id" \
            --no-enable-nvtx
    else
        echo -e "${BLUE}üöÄ Starting standard training (local):${NC}"
        echo "  Model: $model_name"
        echo "  Epochs: $epochs"
        echo "  Batch size: $batch_size"
        echo "  Workers: $num_workers"
        
        uv run python scripts/profile_training.py train \
            --model-name "$model_name" \
            --max-epochs $epochs \
            --batch-size $batch_size \
            --num-workers $num_workers \
            --experiment-name "$job_id"
    fi
}

# Function to list available models locally
list_available_models_local() {
    echo -e "${BLUE}üìã Available Models (local):${NC}"
    
    # Check if uv is available
    if ! command -v uv &> /dev/null; then
        echo -e "${RED}‚ùå uv not found. Please run: $0 setup${NC}"
        return 1
    fi
    
    # Check if virtual environment exists
    if [ ! -d ".venv" ]; then
        echo -e "${YELLOW}‚ö†Ô∏è  Virtual environment not found. Please run: $0 setup${NC}"
        return 1
    fi
    
    uv run python scripts/profile_training.py list-models
}

# Function to set up local development environment with uv (no Docker)
setup_local_env() {
    echo -e "${YELLOW}üõ†Ô∏è  Setting up local development environment with uv...${NC}"
    
    # Check if uv is available
    if ! command -v uv &> /dev/null; then
        echo -e "${RED}‚ùå uv not found. Installing uv...${NC}"
        echo -e "${BLUE}‚ÑπÔ∏è  Installing uv via curl...${NC}"
        curl -LsSf https://astral.sh/uv/install.sh | sh
        
        # Add both possible installation paths to PATH for current session
        export PATH="$HOME/.local/bin:$HOME/.cargo/bin:$PATH"
        
        # Check again
        if ! command -v uv &> /dev/null; then
            echo -e "${RED}‚ùå uv installation failed. Please install manually:${NC}"
            echo "curl -LsSf https://astral.sh/uv/install.sh | sh"
            echo -e "${BLUE}üí° Then add to PATH: export PATH=\"\$HOME/.local/bin:\$PATH\"${NC}"
            echo -e "${BLUE}üí° Or restart your shell to pick up the PATH changes${NC}"
            return 1
        fi
        
        echo -e "${GREEN}‚úÖ uv installed successfully${NC}"
    else
        echo -e "${GREEN}‚úÖ uv is already installed${NC}"
    fi
    
    # Check if pyproject.toml exists
    if [ ! -f "pyproject.toml" ]; then
        echo -e "${RED}‚ùå pyproject.toml not found. Are you in the project root?${NC}"
        return 1
    fi
    
    # Create virtual environment if it doesn't exist
    if [ ! -d ".venv" ]; then
        echo -e "${YELLOW}üîß Creating virtual environment...${NC}"
        uv venv
        echo -e "${GREEN}‚úÖ Virtual environment created at .venv${NC}"
    else
        echo -e "${BLUE}‚ÑπÔ∏è  Virtual environment already exists at .venv${NC}"
    fi
    
    # Install dependencies
    echo -e "${YELLOW}üì¶ Installing project dependencies...${NC}"
    if uv sync; then
        echo -e "${GREEN}‚úÖ Dependencies installed successfully${NC}"
    else
        echo -e "${RED}‚ùå Failed to install dependencies${NC}"
        return 1
    fi
    
    # Install the project in development mode
    echo -e "${YELLOW}üîß Installing project in development mode...${NC}"
    if uv pip install -e .; then
        echo -e "${GREEN}‚úÖ Project installed in development mode${NC}"
    else
        echo -e "${YELLOW}‚ö†Ô∏è  Project installation failed, but dependencies are installed${NC}"
    fi
    
    echo ""
    echo -e "${GREEN}üéâ Local development environment setup complete!${NC}"
    echo ""
    echo -e "${BLUE}üí° To activate the environment:${NC}"
    echo "  source .venv/bin/activate"
    echo ""
    echo -e "${BLUE}üí° To run scripts locally:${NC}"
    echo "  uv run python scripts/profile_training.py --help"
    echo "  uv run python scripts/profile_training.py list-models"
    echo "  uv run python -m src.encoding_101.training.train --help"
    echo ""
    echo -e "${BLUE}üí° Alternative activation (fish shell):${NC}"
    echo "  source .venv/bin/activate.fish"
    echo ""
    echo -e "${BLUE}üí° To deactivate later:${NC}"
    echo "  deactivate"
    echo ""
    echo -e "${YELLOW}üìù Note: For GPU training, you'll need CUDA drivers installed locally${NC}"
    echo -e "${BLUE}‚ÑπÔ∏è  For containerized GPU training, use: $0 train${NC}"
}

# Function to run interactive shell
run_shell() {
    build_image_if_needed
    
    echo -e "${BLUE}üêö Starting interactive shell...${NC}"
    $(get_compose_cmd) run --rm ai-learning bash
}

# Function to clean up Docker resources
docker_cleanup() {
    echo -e "${BLUE}üßπ Docker Cleanup${NC}"
    echo "=================="
    
    # Show current disk usage
    echo -e "${YELLOW}üìä Current Docker disk usage:${NC}"
    docker system df
    echo ""
    
    # Count dangling images
    local dangling_count=$(docker images --filter "dangling=true" -q | wc -l)
    echo -e "${BLUE}üîç Found $dangling_count dangling images (with <none> tag)${NC}"
    
    if [ "$dangling_count" -gt 0 ]; then
        echo -e "${YELLOW}üóëÔ∏è  Removing dangling images...${NC}"
        docker image prune -f
        echo -e "${GREEN}‚úÖ Removed dangling images${NC}"
    else
        echo -e "${GREEN}‚úÖ No dangling images found${NC}"
    fi
    
    echo ""
    
    # Clean up unused containers
    echo -e "${YELLOW}üóëÔ∏è  Removing stopped containers...${NC}"
    docker container prune -f
    
    # Clean up unused networks
    echo -e "${YELLOW}üóëÔ∏è  Removing unused networks...${NC}"
    docker network prune -f
    
    # Clean up unused volumes (be careful with this)
    local volume_count=$(docker volume ls -qf dangling=true | wc -l)
    if [ "$volume_count" -gt 0 ]; then
        echo -e "${YELLOW}‚ö†Ô∏è  Found $volume_count unused volumes${NC}"
        echo -e "${BLUE}üí° To clean volumes: docker volume prune -f${NC}"
        echo -e "${BLUE}üí° (Not auto-cleaned to preserve data)${NC}"
    else
        echo -e "${GREEN}‚úÖ No unused volumes found${NC}"
    fi
    
    echo ""
    echo -e "${YELLOW}üìä Disk usage after cleanup:${NC}"
    docker system df
    
    echo ""
    echo -e "${GREEN}‚úÖ Docker cleanup complete!${NC}"
    echo -e "${BLUE}üí° For aggressive cleanup: docker system prune -a${NC}"
}

# Function to run Hydra-based training (next-generation config management)
run_hydra_training() {
    local args=("$@")
    
    build_image_if_needed
    
    echo -e "${BLUE}üîÆ Hydra-powered training with hierarchical configuration...${NC}"
    echo "  Args: ${args[*]}"
    
    # Build command - pass all arguments directly to the script
    local cmd="python scripts/hydra_training.py"
    
    # Add all arguments
    if [[ ${#args[@]} -gt 0 ]]; then
        cmd="$cmd ${args[*]}"
    fi
    
    echo -e "${YELLOW}üöÄ Running: $cmd${NC}"
    
    $(get_compose_cmd) run --rm ai-learning $cmd
}

# Function to run Hydra-based training locally
run_hydra_training_local() {
    local args=("$@")
    
    echo -e "${BLUE}üè† Running Hydra training locally with uv...${NC}"
    
    # Check if uv is available
    if ! command -v uv &> /dev/null; then
        echo -e "${RED}‚ùå uv not found. Please run: $0 setup${NC}"
        return 1
    fi
    
    # Check if virtual environment exists
    if [ ! -d ".venv" ]; then
        echo -e "${YELLOW}‚ö†Ô∏è  Virtual environment not found. Please run: $0 setup${NC}"
        return 1
    fi
    
    # Build command - pass all arguments directly to the script
    local cmd="python scripts/hydra_training.py"
    
    # Add all arguments
    if [[ ${#args[@]} -gt 0 ]]; then
        cmd="$cmd ${args[*]}"
    fi
    
    echo -e "${YELLOW}üöÄ Running: $cmd${NC}"
    
    uv run $cmd
}

# Parse command line arguments
case "${1:-help}" in
    "train")
        show_docker_banner
        ensure_docker_installed
        ensure_docker_running "train"
        # train [model] [epochs] [batch_size] [--profile] [job_id] [workers]
        if [[ "${5:-}" == "--profile" ]]; then
            run_unified_training "${2:-NVTXVanillaAutoencoder}" "${3:-10}" "${4:-64}" "true" "${6:-training_profile}" "${7:-2}"
        else
            run_unified_training "${2:-NVTXVanillaAutoencoder}" "${3:-10}" "${4:-64}" "false" "${5:-training}" "${6:-0}"
        fi
        ;;
    "train-nvtx")
        show_docker_banner
        ensure_docker_installed
        ensure_docker_running "train-nvtx"
        # Backward compatibility: train-nvtx [epochs] [batch_size] [job_id] [workers] [model] 
        run_unified_training "${6:-NVTXVanillaAutoencoder}" "${2:-5}" "${3:-256}" "true" "${4:-training_profile}" "${5:-2}"
        ;;
    "profile")
        show_docker_banner
        ensure_docker_installed
        ensure_docker_running "profile"
        # Backward compatibility: profile [model] [epochs] [batch_size] [job_id] [workers]
        run_unified_training "${2:-NVTXVanillaAutoencoder}" "${3:-5}" "${4:-256}" "true" "${5:-model_profile}" "${6:-2}"
        ;;
    "list-models")
        show_docker_banner
        ensure_docker_installed
        ensure_docker_running "list-models"
        list_available_models
        ;;
    "export-requirements")
        export_requirements
        ;;
    "check-ngc-packages")
        show_docker_banner
        ensure_docker_installed
        check_ngc_packages
        ;;
    "tensorboard")
        show_docker_banner
        ensure_docker_installed
        ensure_docker_running "tensorboard"
        start_tensorboard
        ;;
    "tensorboard-uv")
        show_local_banner
        start_tensorboard_local
        ;;
    "nsight")
        show_docker_banner
        ensure_docker_installed
        ensure_docker_running "nsight"
        start_nsight_streamer
        ;;
    "nsight-stop")
        show_docker_banner
        ensure_docker_installed
        ensure_docker_running "nsight-stop"
        stop_nsight_streamer
        ;;
    "train-local")
        show_local_banner
        # Pass all arguments except the first one (command name)
        run_hydra_training_local "${@:2}"
        ;;
    "list-models-local")
        show_local_banner
        list_available_models_local
        ;;
    "setup")
        show_local_banner
        setup_local_env
        ;;
    "shell")
        show_docker_banner
        ensure_docker_installed
        ensure_docker_running "shell"
        run_shell
        ;;
    "cleanup")
        show_docker_banner
        ensure_docker_installed
        ensure_docker_running "cleanup"
        docker_cleanup
        ;;
    "install-compose")
        install_docker_compose
        ;;
    "install-nvidia-docker")
        install_nvidia_docker
        ;;
    "hydra-train")
        show_docker_banner
        ensure_docker_installed
        ensure_docker_running "hydra-train"
        # Pass all arguments except the first one (command name)
        run_hydra_training "${@:2}"
        ;;
    "hydra-train-local")
        show_local_banner
        # Pass all arguments except the first one (command name)
        run_hydra_training_local "${@:2}"
        ;;
    "help"|*)
        echo -e "${BLUE}Usage: $0 [COMMAND] [OPTIONS]${NC}"
        echo ""
        echo "Commands:"
        echo "  setup                                     - Set up local development environment with uv (no Docker)"
        echo "  train [model] [epochs] [batch_size] [--profile] [job_id] [workers] - Train autoencoder (default: NVTXVanillaAutoencoder, 10 epochs, 64 batch)"
        echo "  train-local [args...]                     - Run Hydra-based training locally with uv (no Docker)"
        echo "  train-nvtx [epochs] [batch_size] [job_id] [workers] [model] - Legacy NVTX profiling (default: 5 epochs, 256 batch, 2 workers)"
        echo "  profile [model] [epochs] [batch_size] [job_id] [workers] - Legacy model profiling (default: NVTXVanillaAutoencoder, 5 epochs, 256 batch, 2 workers)"
        echo "  list-models                               - List all available autoencoder models"
        echo "  list-models-local                         - List all available autoencoder models (local uv environment)"
        echo "  export-requirements                       - Export requirements.txt excluding NGC pre-installed packages"
        echo "  check-ngc-packages                        - Check what packages are installed in NGC container"
        echo "  tensorboard                               - Start TensorBoard service (Docker)"
        echo "  tensorboard-uv                            - Start TensorBoard locally with uv (no Docker)"
        echo "  nsight                                    - Start NVIDIA Nsight Streamer (web profiling analysis)"
        echo "  nsight-stop                               - Stop NVIDIA Nsight Streamer"
        echo "  shell                                     - Start interactive bash shell"
        echo "  cleanup                                   - Clean up Docker resources"
        echo "  install-compose                           - Manually install Docker Compose"
        echo "  hydra-train [args...]                     - Run Hydra-based training (next-generation config management)"
        echo ""
        echo "  help                                      - Show this help message"
        echo ""
        echo "Examples:"
        echo ""
        echo "  üõ†Ô∏è  Local Development Setup:"
        echo "  $0 setup                                 - Set up local dev environment with uv (recommended first step)"
        echo ""
        echo "  üè† Local Training (No Docker):"
        echo "  $0 train-local                           - Train with default Hydra config locally"
        echo "  $0 train-local model=cnn_autoencoder     - Train CNN model locally using Hydra"
        echo "  $0 train-local training.max_epochs=20    - Train locally with 20 epochs"
        echo "  $0 list-models-local                     - Show all available models (local environment)"
        echo "  $0 tensorboard-uv                        - Start TensorBoard locally on port 6006"
        echo ""
        echo "  üéØ Docker Training (Full Features):"
        echo "  $0 train                                  - Train default model (NVTXVanillaAutoencoder, 10 epochs, 64 batch)"
        echo "  $0 train CNNAutoencoder 20 128           - Train CNN model for 20 epochs with batch size 128"
        echo "  $0 train NVTXVanillaAutoencoder 5 256 --profile gpu_test 2 - Train with NVTX profiling"
        echo "  $0 train VanillaAutoencoder 10 64        - Train vanilla model (no profiling)"
        echo "  $0 tensorboard                           - Start TensorBoard with Docker on port 6007"
        echo ""
        echo "  üîÆ Hydra-Powered Training (Next-Generation Config Management):"
        echo "  $0 hydra-train                           - Train with default Hydra config"
        echo "  $0 hydra-train model=cnn_autoencoder     - Train CNN model using Hydra"
        echo "  $0 hydra-train training.max_epochs=20 model.latent_dim=256 - Override parameters"
        echo "  $0 hydra-train model=nvtx_vanilla_autoencoder,cnn_autoencoder --multirun - Multi-run experiments"
        echo "  $0 hydra-train --config-name=experiment/cnn_comparison - Run specific experiment"
        echo ""
        echo ""
        echo "  ÔøΩÔøΩ Model Discovery:"
        echo "  $0 list-models                           - Show all available models"
        echo ""
        echo "  üì¶ Development Tools:"
        echo "  $0 export-requirements                   - Export filtered requirements.txt for Docker"
        echo ""
        echo "  üîô Legacy Commands (Backward Compatibility):"
        echo "  $0 train-nvtx 5 256                      - NVTX profiling with default model"
        echo "  $0 train-nvtx 3 512 gpu_experiment 4 NVTXCNNAutoencoder - NVTX profiling with CNN model"
        echo "  $0 profile NVTXCNNAutoencoder 10 128     - Profile CNN model"
        echo "  $0 tensorboard                           - Start TensorBoard with Docker on port 6007"
        echo "  $0 nsight                                - Start Nsight Streamer on port 8080"
        echo "  $0 nsight-stop                           - Stop Nsight Streamer"
        echo "  $0 shell                                 - Interactive development shell"
        echo "  $0 cleanup                               - Remove dangling Docker images"
        echo ""
        echo "NVTX Profiling Features:"
        echo "  ‚Ä¢ Comprehensive GPU metrics (Graphics, DRAM, NVLink, PCIe, SM, Tensor Core)"
        echo "  ‚Ä¢ Epoch-level NVTX markers for clear timeline navigation"
        echo "  ‚Ä¢ CPU sampling and context switching tracking"
        echo "  ‚Ä¢ Output saved to ./profiling_output/[job_id]_[epochs]ep.nsys-rep"
        echo ""
        echo "Nsight Streamer Features:"
        echo "  ‚Ä¢ Web-based profiling analysis via browser (http://localhost:8080)"
        echo "  ‚Ä¢ Automatically detects .nsys-rep files from ./profiling_output/"
        echo "  ‚Ä¢ Hardware-accelerated video streaming with NVIDIA GPU"
        echo "  ‚Ä¢ Remote-friendly analysis without large file transfers"
        echo "  ‚Ä¢ Username: nvidia, Password: nvidia (default)"
        echo ""
        echo "Installation:"
        echo "  This script will automatically install Docker, Docker Compose, and NVIDIA Docker runtime"
        echo "  if they are not already installed on your system."
        ;;
esac
